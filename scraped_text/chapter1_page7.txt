TITLE: LLM Course
================================================================================

LLM Course documentation
Ungraded quiz
LLM Course
üè° View all resources
Agents Course
Audio Course
Community Computer Vision Course
Deep RL Course
Diffusion Course
LLM Course
MCP Course
ML for 3D Course
ML for Games Course
Open-Source AI Cookbook
Robotics Course
a smol course
Search documentation
AR
BN
DE
EN
ES
FA
FR
GJ
HE
HI
ID
IT
JA
KO
MY
NE
PL
PT
RO
RU
RUM
TE
TH
TR
VI
ZH-CN
ZH-TW
Join the Hugging Face community
and get access to the augmented documentation experience
Collaborate on models, datasets and Spaces
Faster examples with accelerated inference
Switch between documentation themes
Sign Up
to get started
Copy page
Ungraded quiz
So far, this chapter has covered a lot of ground! Don‚Äôt worry if you didn‚Äôt grasp all the details, but it‚Äôs to reflect on what you‚Äôve learned so far with a quiz.
This quiz is ungraded, so you can try it as many times as you want. If you struggle with some questions, follow the tips and revisit the material. You‚Äôll be quizzed on this material again in the certification exam.
1. Explore the Hub and look for the roberta-large-mnli checkpoint. What task does it perform?
Summarization
Text classification
Text generation
Submit
2. What will the following code return?
Copied
from
transformers
import
pipeline
ner = pipeline(
"ner"
, grouped_entities=
True
)
ner(
"My name is Sylvain and I work at Hugging Face in Brooklyn."
)
It will return classification scores for this sentence, with labels "positive" or "negative".
It will return a generated text completing this sentence.
It will return the words representing persons, organizations or locations.
Submit
3. What should replace ‚Ä¶ in this code sample?
Copied
from
transformers
import
pipeline
filler = pipeline(
"fill-mask"
, model=
"bert-base-cased"
)
result = filler(
"..."
)
This <mask> has been waiting for you.
This [MASK] has been waiting for you.
This man has been waiting for you.
Submit
4. Why will this code fail?
Copied
from
transformers
import
pipeline
classifier = pipeline(
"zero-shot-classification"
)
result = classifier(
"This is a course about the Transformers library"
)
This pipeline requires that labels be given to classify this text.
This pipeline requires several sentences, not just one.
The ü§ó Transformers library is broken, as usual.
This pipeline requires longer inputs; this one is too short.
Submit
5. What does ‚Äútransfer learning‚Äù mean?
Transferring the knowledge of a pretrained model to a new model by training it on the same dataset.
Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.
Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model.
Submit
6. True or false? A language model usually does not need labels for its pretraining.
True
False
Submit
7. Select the sentence that best describes the terms ‚Äúmodel‚Äù, ‚Äúarchitecture‚Äù, and ‚Äúweights‚Äù.
If a model is a building, its architecture is the blueprint and the weights are the people living inside.
An architecture is a map to build a model and its weights are the cities represented on the map.
An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.
Submit
8. Which of these types of models would you use for completing prompts with generated text?
An encoder model
A decoder model
A sequence-to-sequence model
Submit
9. Which of those types of models would you use for summarizing texts?
An encoder model
A decoder model
A sequence-to-sequence model
Submit
10. Which of these types of models would you use for classifying text inputs according to certain labels?
An encoder model
A decoder model
A sequence-to-sequence model
Submit
11. What possible source can the bias observed in a model have?
The model is a fine-tuned version of a pretrained model and it picked up its bias from it.
The data the model was trained on is biased.
The metric the model was optimizing for is biased.
Submit
Update
on GitHub
‚Üê
Transformer Architectures
Inference with LLMs
‚Üí